{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c15d7e-c108-4ee8-adc0-87fc6013c720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching page 1 for 'artificial intelligence' …\n",
      "Cover download failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Searching page 2 for 'artificial intelligence' …\n",
      "Searching page 3 for 'artificial intelligence' …\n",
      "Searching page 4 for 'artificial intelligence' …\n",
      "Searching page 5 for 'artificial intelligence' …\n",
      "Cover download failed: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "Searching page 6 for 'artificial intelligence' …\n",
      "Searching page 7 for 'artificial intelligence' …\n",
      "Searching page 8 for 'artificial intelligence' …\n",
      "Searching page 9 for 'artificial intelligence' …\n",
      "Searching page 10 for 'artificial intelligence' …\n",
      "Saved: ol_data\\openlibrary_books.csv ol_data\\openlibrary_books.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "class OpenLibraryScraper:\n",
    "    def __init__(self, output_dir=\"ol_data\"):\n",
    "        self.output_dir = output_dir\n",
    "        self.images_dir = os.path.join(output_dir, \"images\")\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.images_dir, exist_ok=True)\n",
    "        self.session = requests.Session()\n",
    "        # It is recommended to provide a descriptive User-Agent including contact info\n",
    "        self.session.headers.update({\n",
    "            \"User-Agent\": \"MyOpenLibraryScraper/1.0 (your_email@example.com)\"\n",
    "        })\n",
    "        self.books = []\n",
    "\n",
    "    def search_books(self, query: str, page: int = 1, limit: int = 100):\n",
    "        \"\"\"\n",
    "        Use the OpenLibrary Search API to get search results.\n",
    "        Query example: \"python programming\"\n",
    "        page starts from 1.\n",
    "        \"\"\"\n",
    "        base = \"https://openlibrary.org/search.json\"\n",
    "        params = {\n",
    "            \"q\": query,\n",
    "            \"page\": page,\n",
    "            \"limit\": limit\n",
    "        }\n",
    "        url = f\"{base}?{urlencode(params)}\"\n",
    "        resp = self.session.get(url, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        data = resp.json()\n",
    "        return data\n",
    "\n",
    "    def download_cover(self, cover_id: int, size=\"M\", book_id=None):\n",
    "        \"\"\"\n",
    "        Download cover image from OpenLibrary Covers service.\n",
    "        cover_id: integer id of cover from API `cover_i`\n",
    "        size: \"S\", \"M\", or \"L\"\n",
    "        \"\"\"\n",
    "        if cover_id is None:\n",
    "            return None\n",
    "        # Format: https://covers.openlibrary.org/b/id/<cover_id>-M.jpg\n",
    "        url = f\"https://covers.openlibrary.org/b/id/{cover_id}-{size}.jpg\"\n",
    "        try:\n",
    "            resp = self.session.get(url, timeout=10)\n",
    "            if resp.status_code == 200:\n",
    "                fname = f\"{book_id or cover_id}-{size}.jpg\"\n",
    "                path = os.path.join(self.images_dir, fname)\n",
    "                with open(path, \"wb\") as f:\n",
    "                    f.write(resp.content)\n",
    "                return path\n",
    "        except Exception as e:\n",
    "            print(\"Cover download failed:\", e)\n",
    "        return None\n",
    "\n",
    "    def collect(self, query: str, pages: int = 5, limit_per_page: int = 100, download_covers: bool = True):\n",
    "        \"\"\"\n",
    "        Collect metadata of books matching `query`.\n",
    "        pages * limit_per_page = maximum number of books.\n",
    "        \"\"\"\n",
    "        for p in range(1, pages + 1):\n",
    "            print(f\"Searching page {p} for '{query}' …\")\n",
    "            result = self.search_books(query, page=p, limit=limit_per_page)\n",
    "            docs = result.get(\"docs\", [])\n",
    "            if not docs:\n",
    "                print(\"No docs returned, stopping.\")\n",
    "                break\n",
    "\n",
    "            for doc in docs:\n",
    "                try:\n",
    "                    # Extract fields (use .get safely)\n",
    "                    book = {\n",
    "                        \"title\": doc.get(\"title\"),\n",
    "                        \"author_names\": doc.get(\"author_name\", []),\n",
    "                        \"first_publish_year\": doc.get(\"first_publish_year\"),\n",
    "                        \"edition_count\": doc.get(\"edition_count\"),\n",
    "                        \"cover_id\": doc.get(\"cover_i\"),\n",
    "                        \"key\": doc.get(\"key\"),  # work key like \"/works/OL12345W\"\n",
    "                        \"isbn\": doc.get(\"isbn\", []),\n",
    "                        \"subject\": doc.get(\"subject\", []),\n",
    "                    }\n",
    "\n",
    "                    # Download cover image\n",
    "                    if download_covers and book[\"cover_id\"] is not None:\n",
    "                        img_path = self.download_cover(book[\"cover_id\"], size=\"M\", book_id=book[\"key\"].strip(\"/\").replace(\"/\", \"_\"))\n",
    "                        book[\"cover_path\"] = img_path\n",
    "                    else:\n",
    "                        book[\"cover_path\"] = None\n",
    "\n",
    "                    self.books.append(book)\n",
    "                except Exception as e:\n",
    "                    print(\"Error processing doc:\", e)\n",
    "                    continue\n",
    "\n",
    "            time.sleep(0.5)  # polite delay between API calls\n",
    "\n",
    "    def save(self):\n",
    "        df = pd.DataFrame(self.books)\n",
    "        csv_path = os.path.join(self.output_dir, \"openlibrary_books.csv\")\n",
    "        json_path = os.path.join(self.output_dir, \"openlibrary_books.json\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        df.to_json(json_path, orient=\"records\", indent=2)\n",
    "        print(\"Saved:\", csv_path, json_path)\n",
    "\n",
    "def main():\n",
    "    scraper = OpenLibraryScraper()\n",
    "    scraper.collect(query=\"artificial intelligence\", pages=10, limit_per_page=100, download_covers=True)\n",
    "    scraper.save()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4684074c-c517-4cf4-92d2-c51e83acfd0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
